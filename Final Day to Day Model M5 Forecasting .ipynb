{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression,BayesianRidge, Lasso\n",
    "from statistics import mean\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "import datetime\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    calendar = pd.read_csv('calendar.csv')\n",
    "    calendar = reduce_mem_usage(calendar)\n",
    "    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n",
    "    sell_prices = pd.read_csv('sell_prices.csv')\n",
    "    sell_prices = reduce_mem_usage(sell_prices)\n",
    "    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n",
    "    sales_train_validation = pd.read_csv('sales_train_validation.csv')\n",
    "    print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "    submission = pd.read_csv('sample_submission.csv')\n",
    "    return calendar, sell_prices, sales_train_validation, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 55000000, merge = False):\n",
    "    \n",
    "    # melt sales data, get it ready for training\n",
    "    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "    print('Melted sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n",
    "    sales_train_validation = reduce_mem_usage(sales_train_validation)\n",
    "    \n",
    "    # seperate test dataframes\n",
    "    test1_rows = [row for row in submission['id'] if 'validation' in row]\n",
    "    test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n",
    "    test1 = submission[submission['id'].isin(test1_rows)]\n",
    "    test2 = submission[submission['id'].isin(test2_rows)]\n",
    "    \n",
    "    # change column names\n",
    "    test1.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931', \n",
    "                      'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941']\n",
    "    test2.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', 'd_1950', 'd_1951', 'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', 'd_1959', \n",
    "                      'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', 'd_1968', 'd_1969']\n",
    "    \n",
    "    # get product table\n",
    "    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "    \n",
    "    # merge with product table\n",
    "    test2['id'] = test2['id'].str.replace('_evaluation','_validation')\n",
    "    test1 = test1.merge(product, how = 'left', on = 'id')\n",
    "    test2 = test2.merge(product, how = 'left', on = 'id')\n",
    "    test2['id'] = test2['id'].str.replace('_validation','_evaluation')\n",
    "    \n",
    "    test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "    test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n",
    "    \n",
    "    sales_train_validation['part'] = 'train'\n",
    "    test1['part'] = 'test1'\n",
    "    test2['part'] = 'test2'\n",
    "    \n",
    "    data = pd.concat([sales_train_validation, test1, test2], axis = 0)\n",
    "    \n",
    "    del sales_train_validation, test1, test2\n",
    "    \n",
    "    data = data.loc[nrows:]\n",
    "    \n",
    "    calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n",
    "    \n",
    "    # delete test2 for now, don't delete when we do next stage of testing in June \n",
    "    data = data[data['part'] != 'test2']\n",
    "    \n",
    "    if merge:\n",
    "        data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n",
    "        data.drop(['day'], inplace = True, axis = 1)\n",
    "        data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "        print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n",
    "    else: \n",
    "        pass\n",
    "\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "calendar, sell_prices, sales_train_validation, submission = read_data()\n",
    "\n",
    "data = melt_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 35000000, merge = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def truncate(data, start_date):\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"])\n",
    "\n",
    "    mask = (data['date'] > start_date)\n",
    "    data = data.loc[mask]\n",
    "    data.head()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del calendar, sell_prices, sales_train_validation, submission\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def transform(data):\n",
    "    \n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        data[feature].fillna('unknown', inplace = True)\n",
    "        \n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in cat:\n",
    "        encoder = LabelEncoder()\n",
    "        data[feature] = encoder.fit_transform(data[feature])\n",
    "        \n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    time_features = ['year', 'month', 'quarter', 'week', 'day', 'dayofweek', 'dayofyear']\n",
    "    dtype = np.int16\n",
    "    for time_feature in time_features:\n",
    "        data[time_feature] = getattr(data['date'].dt, time_feature).astype(dtype)\n",
    "        \n",
    "    data = reduce_mem_usage(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = truncate(data, '2014-04-01')\n",
    "data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(data):\n",
    "    weights = pd.read_csv(\"weight_scale_1914.csv\")\n",
    "    \n",
    "    weights = weights[[ 'Level_id', 'Agg_Level_1', 'Agg_Level_2','weight', 'scale']]\n",
    "    level_12 = weights[weights.Level_id == 'Level12']\n",
    "    \n",
    "    level_12['scaled_weight'] = level_12.weight/np.sqrt(level_12.scale)\n",
    "    level_12[\"combined\"] = level_12[\"Agg_Level_1\"] + '_' + level_12[\"Agg_Level_2\"] + \"_validation\"\n",
    "\n",
    "    temp_weights = level_12[['combined', 'scaled_weight']]\n",
    "    data = pd.merge(data, temp_weights, how = 'left', left_on = 'id', right_on = 'combined')\n",
    "    \n",
    "    del level_12, weights\n",
    "    gc.collect()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_weights(data)\n",
    "data = reduce_mem_usage(data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('sales_train_evaluation.csv')\n",
    "train_df.drop(train_df.columns[6:1165], axis=1, inplace = True)\n",
    "d_cols = [col for col in train_df.columns if 'd_' in col]\n",
    "rec = (train_df[d_cols].values)\n",
    "\n",
    "\n",
    "def add_lags(grid_df, shift):\n",
    "    \n",
    "    lags = [0, 1, 2, 3, 4, 5, 6, 7, 28]\n",
    "    start_time = time()\n",
    "    print( 72 * '#', '\\nAdding lag columns')\n",
    "    for i in lags:\n",
    "        grid_df[f'lag_{i}'] = grid_df['demand'].shift(30490 * (i + shift)).astype(np.float16)\n",
    "        grid_df[f'price_lag_{i}'] = grid_df['sell_price'].shift(30490 * (i + shift)).astype(np.float16)\n",
    "        if (i == 0):\n",
    "            grid_df[f'event_name1_lag_{i}'] = grid_df['event_name_1'].shift(30490 * (i + shift)).astype(np.float16)\n",
    "            grid_df[f'event_type1_lag_{i}'] = grid_df['event_type_1'].shift(30490 * (i + shift)).astype(np.float16)\n",
    "            grid_df[f'event_name2_lag_{i}'] = grid_df['event_name_2'].shift(30490 * (i + shift)).astype(np.float16)\n",
    "            grid_df[f'event_type2_lag_{i}'] = grid_df['event_type_2'].shift(30490 * (i + shift)).astype(np.float16)\n",
    "            \n",
    "            #grid_df['is_weekend_lag'] = grid_df['is_weekend'].shift(30490 * (i + shift)).astype(np.float16)\n",
    "        if (i == 7):\n",
    "            grid_df[f'event_name1_lag_{i}'] = grid_df['event_name_1'].shift(30490 * (i + shift)).astype(np.float16)\n",
    "            grid_df[f'event_type1_lag_{i}'] = grid_df['event_type_1'].shift(30490 * (i + shift)).astype(np.float16)\n",
    "            grid_df[f'event_name2_lag_{i}'] = grid_df['event_name_2'].shift(30490 * (i + shift)).astype(np.float16)\n",
    "            grid_df[f'event_type2_lag_{i}'] = grid_df['event_type_2'].shift(30490 * (i + shift)).astype(np.float16)\n",
    "            grid_df[f'event_name1_future'] = grid_df['event_name_1'].shift(-30490 * (i)).astype(np.float16)\n",
    "            grid_df[f'event_type1_future'] = grid_df['event_type_1'].shift(-30490 * (i)).astype(np.float16)\n",
    "            grid_df[f'event_name2_future'] = grid_df['event_name_2'].shift(-30490 * (i)).astype(np.float16)\n",
    "            grid_df[f'event_type2_future'] = grid_df['event_type_2'].shift(-30490 * (i)).astype(np.float16)\n",
    "            #grid_df[f'event_name1_past'] = grid_df['event_name_1'].shift(30490 * (i)).astype(np.float16)\n",
    "            #grid_df[f'event_type1_past'] = grid_df['event_type_1'].shift(30490 * (i)).astype(np.float16)\n",
    "            #grid_df[f'event_name2_past'] = grid_df['event_name_2'].shift(30490 * (i)).astype(np.float16)\n",
    "            #grid_df[f'event_type2_past'] = grid_df['event_type_2'].shift(30490 * (i)).astype(np.float16)\n",
    "    \n",
    "    print(f'Time: {(time() - start_time):.2f} seconds')\n",
    "        \n",
    "        \n",
    "############################################################       \n",
    "################# Rolling window columns ###################\n",
    "\n",
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "\n",
    "def make_rolling_col(rw, window, function, weights = False): \n",
    "    # We need to take off the last columns to\n",
    "    # get the rolling feature shifted one day.\n",
    "    \n",
    "    split_rw = np.split(rw, 10, axis=0)\n",
    "    split_col = [function(rw, -1) for rw in split_rw]\n",
    "    col = np.concatenate(split_col)\n",
    "    col = col[:, :-1].T.reshape(-1,)\n",
    "\n",
    "    # The new column must be prepended with np.nans \n",
    "    # to account for missing gaps\n",
    "    \n",
    "    \n",
    "    weight_7 = [1, 1, 1, 1, 1, 1, 3]\n",
    "    weight_14 = [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3]\n",
    "    weight_28 = [1, 1, 1, 1, 1, 1, 1.2, 1, 1, 1, 1, 1, 1, 1.5, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3]\n",
    "    if (weights == True):\n",
    "        if (window == 7):\n",
    "            split_rw = np.split(rw, 10, axis=0)\n",
    "            split_col = [function(rw, -1, weights = weight_7) for rw in split_rw]\n",
    "            col = np.concatenate(split_col)\n",
    "            col = col[:, :-1].T.reshape(-1,)\n",
    "        elif (window == 14):\n",
    "            split_rw = np.split(rw, 10, axis=0)\n",
    "            split_col = [function(rw, -1, weights = weight_14) for rw in split_rw]\n",
    "            col = np.concatenate(split_col)\n",
    "            col = col[:, :-1].T.reshape(-1,)\n",
    "        else:\n",
    "            split_rw = np.split(rw, 10, axis=0)\n",
    "            split_col = [function(rw, -1, weights = weight_28) for rw in split_rw]\n",
    "            col = np.concatenate(split_col)\n",
    "            col = col[:, :-1].T.reshape(-1,)\n",
    "            \n",
    "            \n",
    "    return np.append(np.zeros(30490 * window) + np.nan, col).astype(np.float16)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_rolling_cols(df: pd.DataFrame, rec: np.array, windows: list, weighted_windows, functions: list, function_names: list): \n",
    "    \"\"\"Adds rolling features to df.\"\"\"\n",
    "    \n",
    "    print( 72 * '#', '\\nAdding rolling columns\\n',  )\n",
    "    start_time = time()\n",
    "    f = list(zip(functions, function_names))\n",
    "    \n",
    "    for window in windows: \n",
    "        rw = rolling_window(rec, window)\n",
    "        for function in f: \n",
    "            s_time = time()\n",
    "            if (function[1] != 'weighted_mean'):\n",
    "                df[f'shift_1_rolling_{function[1]}_{str(window)}'] = make_rolling_col(rw, window, function[0])\n",
    "                print(f'{function[1]} with window {window} time: {(time() - s_time):.2f} seconds')\n",
    "    \n",
    "    for window in weighted_windows: \n",
    "        rw = rolling_window(rec, window)\n",
    "        for function in f: \n",
    "            s_time = time()\n",
    "            if (function[1] == 'weighted_mean'):\n",
    "                df[f'shift_1_rolling_{function[1]}_{str(window)}'] = make_rolling_col(rw, window, function[0], weights = True)\n",
    "                print(f'{function[1]} with window {window} time: {(time() - s_time):.2f} seconds')\n",
    "                \n",
    "                \n",
    "    print(f'Total time for rolling cols: {(time() - start_time)/60:.2f}')\n",
    "    \n",
    "    \n",
    "    \n",
    "############################################################       \n",
    "################# Shifting function ########################\n",
    "def add_shift_cols(grid_df, shifts, cols, num_series=30490): \n",
    "    \n",
    "    print( 72 * '#', '\\nAdding shift columns',  )\n",
    "    start_time = time()\n",
    "    for shift in shifts: \n",
    "        for col in cols: \n",
    "            grid_df[f\"{col.replace('shift_1', f'shift_{shift}')}\"] = grid_df[col].shift((shift - 1) * num_series)\n",
    "    print(f'Time: {(time() - start_time):.2f} seconds')\n",
    "\n",
    "\n",
    "         \n",
    "            \n",
    "############################################################       \n",
    "################# Create lags df ###########################\n",
    "def make_lags_df_day(data, rec, day): \n",
    "    \n",
    "    start_time = time()\n",
    "\n",
    "    \n",
    "    window = day\n",
    "    add_lags(data, window)\n",
    "    add_rolling_cols(data, \n",
    "                     rec, \n",
    "                     windows=[7, 14, 28],\n",
    "                     weighted_windows = [7, 14, 28], \n",
    "                     functions=[np.mean, np.std, np.average], \n",
    "                     function_names=['mean', 'std', 'weighted_mean'])\n",
    "    \n",
    "    \n",
    "    shifts = [day]\n",
    "   \n",
    "    group = data.groupby('id')['demand']\n",
    "    data['shift_28_rolling_std_7'] = group.transform(lambda x: x.shift(day).rolling(7).std())\n",
    "    data['shift_28_rolling_std_14'] = group.transform(lambda x: x.shift(day).rolling(14).std())\n",
    "    data['shift_28_rolling_std_28'] = group.transform(lambda x: x.shift(day).rolling(28).std())\n",
    "    #data['revenue_0'] = data['price_lag_0'] * data['lag_0']\n",
    "    ocols = [f'shift_1_rolling_mean_{i}' for i in [7, 14, 28]]\n",
    "    add_shift_cols(data, shifts, ocols, num_series=30490)\n",
    "    oocols = [f'shift_1_rolling_weighted_mean_{i}' for i in [7, 14, 28]]\n",
    "    add_shift_cols(data, shifts, oocols, num_series=30490)\n",
    "    \n",
    "    print(72 * '#', f'Total time: {(time() - start_time)//60:} : {(time() - start_time)%60:.2f}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics \n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "def run_lgb_daily(data, cat_indices, rec = rec):\n",
    "    \n",
    "    # reset_index\n",
    "    #data.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # going to evaluate with the last 28 days, try Kfold TSS at some point \n",
    "    \n",
    "    #x_train = data[data['date'] <= '2016-04-24']\n",
    "    #y_train = x_train['demand']\n",
    "    #x_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n",
    "    #y_val = x_val['demand']\n",
    "    test = data[data['date'] >= '2016-04-25']\n",
    "\n",
    "    #train_weights = x_train['scaled_weight']\n",
    "    #val_weights = x_val['scaled_weight']\n",
    "\n",
    "\n",
    "    # define random hyperparammeters\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_jobs': -1,\n",
    "        'seed': 42,   \n",
    "        'learning_rate': 0.07,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'bagging_freq': 1, \n",
    "        'colsample_bytree': 0.85,\n",
    "        'colsample_bynode': 0.85,\n",
    "        'min_data_per_leaf': 25,\n",
    "        'num_leaves': 200,\n",
    "        'lambda_l1': 0.2,\n",
    "        'lambda_l2': 0.2,\n",
    "        'objective': 'tweedie',\n",
    "        'metric': 'rmse',\n",
    "        \"tweedie_variance_power\":1.1}\n",
    "    \n",
    "    new_params = {'boosting_type': 'gbdt',\n",
    "            'seed': 42,\n",
    "            'num_leaves': 2047,\n",
    "            'max_bin': 3050,\n",
    "            'n_estimators': 4000,\n",
    "            'verbose': 20,\n",
    "            'bagging_fraction': 0.85,\n",
    "            'bagging_freq': 1, \n",
    "            'colsample_bytree': 0.85,\n",
    "            'colsample_bynode': 0.85,\n",
    "            'lambda_l1': 0.2,\n",
    "            'lambda_l2': 0.2, \n",
    "            'objective': 'tweedie',\n",
    "            'metric': 'rmse',\n",
    "            \"tweedie_variance_power\":1.1}\n",
    "    \n",
    "    \n",
    "    TS = 30490\n",
    "    #date_list = x_train['date'].unique()\n",
    "    \n",
    "    #test = data[data['date'] >= '2016-04-25']\n",
    "    \n",
    "    \n",
    "    for j in range(1, 29):\n",
    "        \n",
    "        make_lags_df_day(data, rec, j)\n",
    "        gc.collect()\n",
    " \n",
    "        \n",
    "        dates = data['date'].unique()\n",
    "        starting_date = dates[0]\n",
    "        del dates\n",
    "        \n",
    "        print(j)\n",
    "        \n",
    "        start_date = starting_date + pd.to_timedelta(28 + j, unit=\"D\")\n",
    "        end_test = datetime.strptime('2016-04-24', '%d/%m/%y') + pd.to_timedelta(j, unit=\"D\")\n",
    "        \n",
    "        temp_test = data[(data['date'] >= '2016-04-25') & (data['date'] <= end_test)]\n",
    "        x_train = data[(data['date'] >= start_date) & (data['date'] <= '2016-03-27')] \n",
    "        x_val = data[(data['date'] >= '2016-03-28') & (data['date'] <= '2016-04-24')] \n",
    "        \n",
    "        preds = np.zeros(len(temp_test))\n",
    "    \n",
    "        train_days = x_train['date'].nunique()\n",
    "        train_list = []\n",
    "        for a in range(30490):\n",
    "                index = a\n",
    "                train_list.append(a)\n",
    "                for b in range(train_days - 1):\n",
    "                    index += (30490)\n",
    "                    train_list.append(index)\n",
    "        \n",
    "        x_train = x_train.reset_index()\n",
    "        x_train = x_train.reindex(train_list)\n",
    "        x_train = x_train.reset_index()\n",
    "        del train_list\n",
    "        \n",
    "        \n",
    "        val_days = x_val['date'].nunique()\n",
    "        val_list = []\n",
    "        for c in range(30490):\n",
    "                index = c\n",
    "                val_list.append(c)\n",
    "                for d in range(val_days - 1):\n",
    "                    index += (30490)\n",
    "                    val_list.append(index)\n",
    "        \n",
    "        x_val = x_val.reset_index()\n",
    "        x_val = x_val.reindex(val_list)\n",
    "        x_val = x_val.reset_index()\n",
    "        del val_list\n",
    "       \n",
    "\n",
    "        print(f'Training fold for day {j}')\n",
    "    \n",
    "        \n",
    "        i = 3\n",
    "        #val_filter = (-70 + 14 * i) * TS\n",
    "        #val_stopper = (-42 + 14 * i) * TS\n",
    "\n",
    "        #train_fold_df = training_df.iloc[:, :(-98 + 14 * i)]\n",
    "        #valid_fold_df = training_df.iloc[:, (-98 + 14 * i):(-70 + 14 * i)].copy()\n",
    "        #w = WRMSSEForLightGBM(train_fold_df, valid_fold_df, temp_calendar, temp_prices)\n",
    "\n",
    "        features = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', \n",
    "            'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'year', \n",
    "                'month', 'quarter', 'week', 'day', 'dayofweek', 'dayofyear',\n",
    "             'lag_0', 'lag_1','lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'lag_28', \n",
    "             'season',\n",
    "            f'shift_{i}_rolling_mean_7',\n",
    "            f'shift_{i}_rolling_mean_14',\n",
    "            f'shift_{i}_rolling_mean_28',f'shift_{i}_rolling_std_7',\n",
    "            f'shift_{i}_rolling_std_14',\n",
    "            f'shift_{i}_rolling_std_28',\n",
    "            f'shift_{i}_rolling_weighted_mean_7',\n",
    "            f'shift_{i}_rolling_weighted_mean_14',\n",
    "            f'shift_{i}_rolling_weighted_mean_28', 'event_name1_lag_0', 'event_type1_lag_0'\n",
    "             , 'event_name1_lag_7', 'event_type1_lag_7', 'event_name2_lag_0', 'event_type2_lag_0'\n",
    "             , 'event_name2_lag_7', 'event_type2_lag_7', 'price_lag_0', 'price_lag_1','price_lag_2', 'price_lag_3', \n",
    "             'price_lag_4', 'price_lag_5', 'price_lag_6',\n",
    "             'price_lag_7', 'price_lag_28', 'event_name1_future', 'event_type1_future', 'event_name2_future', 'event_type2_future']\n",
    "        \n",
    "        train_set = lgb.Dataset(x_train[features], label = x_train['demand'], \n",
    "                                categorical_feature = cat_indices, feature_name = features, \n",
    "                                weight = x_train[\"scaled_weight\"]) \n",
    "\n",
    "\n",
    "        val_set = lgb.Dataset(x_val[features], \n",
    "                              label = x_val['demand'], \n",
    "                              categorical_feature = cat_indices, feature_name = features, \n",
    "                              weight = x_val[\"scaled_weight\"])\n",
    "        \n",
    "\n",
    "        model = lgb.train(new_params, train_set, num_boost_round = 4000, \n",
    "                              valid_sets = [train_set, val_set], verbose_eval = 20, \n",
    "                          early_stopping_rounds = 150)\n",
    "        \n",
    "    \n",
    "        preds += (model.predict(temp_test[features]))\n",
    "      \n",
    "        \n",
    "            \n",
    "        print('-'*50)\n",
    "        print('\\n')\n",
    "        \n",
    "        temp_test = temp_test[['id', 'date', 'demand']]\n",
    "        temp_test['demand'] = preds\n",
    "        temp_test.to_csv(f'test_{j}(80).csv')\n",
    "        del x_train, temp_test\n",
    "        \n",
    "        \n",
    "        del data[f'shift_{i}_rolling_mean_7'], data[f'shift_{i}_rolling_mean_14'], data[f'shift_{i}_rolling_mean_28'], data[f'shift_{i}_rolling_weighted_mean_7'], data[f'shift_{i}_rolling_weighted_mean_14'], data[f'shift_{i}_rolling_weighted_mean_28'], data[f'shift_{i}_rolling_std_7'], data[f'shift_{i}_rolling_std_14'], data[f'shift_{i}_rolling_std_28'], data['shift_1_rolling_mean_7'], data['shift_1_rolling_mean_14'], data['shift_1_rolling_mean_28'], data['shift_1_rolling_std_7'], data['shift_1_rolling_std_14'], data['shift_1_rolling_std_28']\n",
    "        \n",
    "        gc.collect()\n",
    "        model.save_model(f\"model_1_day{j}.lgb\")\n",
    "    \n",
    "    test1 = pd.read_csv('test_1(80).csv')\n",
    "    test2 = pd.read_csv('test_2(80).csv')\n",
    "    test3 = pd.read_csv('test_3(80).csv')\n",
    "    test4 = pd.read_csv('test_4(80).csv')\n",
    "    test5 = pd.read_csv('test_5(80).csv')\n",
    "    test6 = pd.read_csv('test_6(80).csv')\n",
    "    test7 = pd.read_csv('test_7(80).csv')\n",
    "    test8 = pd.read_csv('test_8(80).csv')\n",
    "    test9 = pd.read_csv('test_9(80).csv')\n",
    "    test10 = pd.read_csv('test_10(80).csv')\n",
    "    test11 = pd.read_csv('test_11(80).csv')\n",
    "    test12 = pd.read_csv('test_12(80).csv')\n",
    "    test13 = pd.read_csv('test_13(80).csv')\n",
    "    test14 = pd.read_csv('test_14(80).csv')\n",
    "    test15 = pd.read_csv('test_15(80).csv')\n",
    "    test16 = pd.read_csv('test_16(80).csv')\n",
    "    test17 = pd.read_csv('test_17(80).csv')\n",
    "    test18 = pd.read_csv('test_18(80).csv')\n",
    "    test19 = pd.read_csv('test_19(80).csv')\n",
    "    test20 = pd.read_csv('test_20(80).csv')\n",
    "    test21 = pd.read_csv('test_21(80).csv')\n",
    "    test22 = pd.read_csv('test_22(80).csv')\n",
    "    test23 = pd.read_csv('test_23(80).csv')\n",
    "    test24 = pd.read_csv('test_24(80).csv')\n",
    "    test25 = pd.read_csv('test_25(80).csv')\n",
    "    test26 = pd.read_csv('test_26(80).csv')\n",
    "    test27 = pd.read_csv('test_27(80).csv')\n",
    "    test28 = pd.read_csv('test_28(80).csv')\n",
    "\n",
    "    test = pd.concat([test1, test2, test3, test4, test5, test6, test7, test8, test9, test10, test11, test12, test13, test14, test15, test16, test17, test18, test19, test20, test21, test22, test23, test24, test25, test26, test27, test28], ignore_index = True)\n",
    "\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(test, submission):\n",
    "    predictions = test[['id', 'date', 'demand']]\n",
    "    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n",
    "    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n",
    "    evaluation = submission[submission['id'].isin(evaluation_rows)]\n",
    "\n",
    "    validation = submission[['id']].merge(predictions, on = 'id')\n",
    "    final = pd.concat([validation, evaluation])\n",
    "    final.to_csv(f'model_68.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission = reduce_mem_usage(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_finals = ['2014-06-05', '2014-06-08', '2014-06-10', '2014-06-12', '2014-06-15', '2015-06-04', '2015-06-07', \n",
    "             '2015-06-09', '2015-06-11', '2015-06-14', '2015-06-16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['LeBron'] = np.zeros(len(data))\n",
    "#for nba in nba_finals:\n",
    "    #data.loc[data.date==nba, 'LeBron'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_events(data):\n",
    "    father_date = '2014-06-15'\n",
    "    data.loc[data.date==father_date, 'event_name_1'] = 7\n",
    "    data.loc[data.date==father_date, 'event_type_1'] = 0\n",
    "    nba_2014_start = '2014-06-05'\n",
    "    data.loc[data.date==nba_2014_start, 'event_name_1'] = 30\n",
    "    data.loc[data.date==nba_2014_start, 'event_type_1'] = 4\n",
    "    nba_2015_start = '2015-06-04'\n",
    "    data.loc[data.date==nba_2015_start, 'event_name_1'] = 30\n",
    "    data.loc[data.date==nba_2015_start, 'event_type_1'] = 4\n",
    "    nba_2015_end = '2015-06-16'\n",
    "    data.loc[data.date==nba_2015_end, 'event_name_1'] = 30\n",
    "    data.loc[data.date==nba_2015_end, 'event_type_1'] = 4\n",
    "    \n",
    "    return data\n",
    "    \n",
    "#data = change_events(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reduce_mem_usage(data)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "test = run_lgb_weekly(data, cat_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
